---
layout: post
title:  "[Letâ€™s Review] Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models + Jax Implementation"
date:   2024-03-07 00:00:00 +0000
categories: recurrent-neural-networks
usemathjax: true
---

In one of the latest paper, Google DeepMind unveiled an novel efficient RNN architecture with local attention. In paper, authors proposed two different models: Hawk, an RNN with gated linear recurrences, and alongside, Griffin, a hybrid model that mixes gated linear recurrences with local attention.

In my Medium blog post, I review that paper and serving Jax implementation of this architecture. 

[Let's Review Post](https://medium.com/@simudt/lets-review-griffin-mixing-gated-linear-recurrences-with-local-attention-for-efficient-lms-57c824846b3d)

[GitHub Repository](https://github.com/simudt/Griffin-Jax)